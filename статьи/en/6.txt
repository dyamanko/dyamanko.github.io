See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/337325658



Dealing with Comprehension and Bugs in Native and Cross-Platform Apps: A
Controlled Experiment

Chapter · November 2019
DOI: 10.1007/978-3-030-35333-9_53




CITATION                                                                                                  READS
1                                                                                                         229


4 authors:

            Maria Caulo                                                                                              Rita Francese
            Università degli Studi della Basilicata                                                                  Università degli Studi di Salerno
            13 PUBLICATIONS 47 CITATIONS                                                                             121 PUBLICATIONS 1,284 CITATIONS

                SEE PROFILE                                                                                               SEE PROFILE



            Giuseppe Scanniello                                                                                      Antonio Spera
            Università degli Studi di Salerno                                                                        Università degli Studi di Salerno
            245 PUBLICATIONS 2,799 CITATIONS                                                                         2 PUBLICATIONS 4 CITATIONS

                SEE PROFILE                                                                                               SEE PROFILE




Some of the authors of this publication are also working on these related projects:


              UML and source code comprehensibility View project



              concern localization View project




 All content following this page was uploaded by Maria Caulo on 11 August 2020.

 The user has requested enhancement of the downloaded file.
Dealing with Comprehension and Bugs in Native
    and Cross-Platform Apps: A Controlled
                 Experiment

    Maria Caulo1 , Rita Francese2 , Giuseppe Scanniello1 , and Antonio Spera2
                       1
                        University of Basilicata, Potenza, Italy
                  {maria.caulo; giuseppe.scanniello}@unibas.it
                     2
                       University of Salerno, Fisciano (SA), Italy
                 francese@unisa.it,a.spera18@studenti.unisa.it



       Abstract. In this paper, we present the results of a controlled exper-
       iment aimed to investigate whether there is a difference when compre-
       hending apps implemented with either cross-platform (Ionic-Cordova-
       Angular) and native (Android) technologies. We divided participants into
       two groups. The participants in each group were asked to comprehend the
       source code of either the app implemented using Ionic-Cordova-Angular
       technology or its Android version. We also asked the participants to
       identify and fix faults in the source code. The goal was to verify if the
       technology might play a role in the execution of these two kinds of tasks.
       We also investigated the affective reactions of participants and the dif-
       ficulty they perceived when accomplishing the tasks mentioned before.
       The most important take-away result is: there is not a statistically signif-
       icant difference in the comprehension and in the identification and fixing
       of bugs when dealing with either native or cross-platform apps.

       Keywords: Android · Cross-platform · Ionic · Sentiment Analysis


1    Introduction
All the software organizations have to afford the problem of developing the
same mobile application several times for different mobile operating systems
(i.e., Android and iOS) and running on many target devices, while preserving
the performances and the user interface interaction of the native approaches. It
is not possible to share code among the various implementations, which have
to be separately developed. As a result, the development process might take a
longer time. Also, maintenance is very expensive because all the maintenance
activities have to be simultaneously conducted on all the software variants.
    To try to reduce the development and maintenance costs and time-to-market
many mobile cross-platform development approaches have been proposed, some
of them are still in development phase [7]. Their main advantage is that the apps
for several mobile platforms are developed and maintained one time. In addi-
tion, many cross-platform tools are based on web technologies and this avoids
web developers to be forced to learn many new languages and development
2       Caulo, Maria et al.

environments. Another reason for choosing cross-platform frameworks is that
they can be adopted for rapid prototyping of apps to be run in various hard-
ware/software platforms. It is also a quick way to be operative in the market
and to reach the maximum number of users with the plan of re-implementing
or migrating them towards native platforms. Research work is focused on the
comparison of the performances of cross-platform apps and the native ones by
evaluating the performances of the apps on different platforms and their User
Experience [19, 25]. In the recent past these results where still in favor of na-
tive platforms, especially in case of applications stressing the user interface or
massively exploiting the hardware resources, such as games. For this kind of ap-
plications cross-platform technology could be lagging and/or producing a worst
User Experience. At present, the adoption of cross-development technologies is
increasing due to the higher performances of top-level devices and we can expect
they will overcome their limitations.
    In this paper, we present the results of a controlled experiment aimed to
investigate whether there is a difference when comprehending apps implemented
with either cross-platform (Ionic-Cordova-Angular) and native (Android) tech-
nologies. Source-code comprehension is vital to deal with many software en-
gineering tasks concerning existing code, e.g., testing [3]. For example, before
testing source code a developer should first comprehend it and then identify
the bug described in a bug report. On this respect, we decided to study also
if there is an effect of cross-platform and native technologies on bug identifica-
tion and fixing. To complete our study, we investigate the affective reactions of
participants and the difficulty they perceived when accomplishing source-code
comprehension and bug identification and fixing.
    This paper is organized as follows: Section 2 discusses related work; Section 3
presents the planning of the controlled experiment, while Section 6 summarizes
the experiment results. Final remarks conclude the the paper in Section 6.


2   Related Work

The greater part of the empirical research in the context of cross-platform apps
is either on the perspective of the end-user while using this kind of apps or on
the analysis of their performances. As an example, the authors in [25] and [19]
compared the application performances of native and cross-platform apps, such
as disk space and battery usage. Results that are negative for cross-platform
solutions at the beginning improve for them in the years because the mobile
device resources increased. Corral et al. [5] evaluated app performances by con-
sidering: hardware access, access to native features such as accelerometer and
Network access. The study was focused on PhoneGap and Android apps. Results
revealed that generally, Android performs better, but that the difference may be
not relevant for general-purpose business applications.
    Heitkotter et al. [9] provided a set of criteria for comparing cross-platform and
native apps, e.g., license and cost, supported platforms, application speed, and
scalability. The criteria were founded on the authors’ and professional developers’
      Dealing with Comprehension and Bugs in Native and Cross-Platform Apps        3

  opinions. On the other hand, Dalmasso et al. [6] provided several decision criteria
  for selecting the appropriate cross-platform technology, including quality of the
  User Experience, the potential users, the security of the app, supportability,
  easiness of updating and time to market. Performances have been evaluated by
  developing Android test applications using four different cross-platform tools.
  Differently, Malavolta et al. [17] analyzed hybrid mobile apps from the end-users
  perspective by mining reviews from the Google Play Store. Results suggested
  that hybrid development is more suitable for data-intensive mobile apps, whereas
  it got poor performances when the app exploits platform-specific features.
      Differently from the research highlighted before, Que et al. [19] approached
  cross-platform development from the developer’s point of view. The authors con-
  sidered aspects related to the easiness of coding, debugging and testing. Easiness
  of use is discussed together with performances. This paper represents the closest
  to ours. One of the most important differences concerns the method to investi-
  gate the defined research questions. In particular, the authors did not founded
  their research on users’ study.
      Our research improves the body of knowledge in the context of cross-platform
  development because we empirically investigated—through a controlled exper-
  iment and quantitative data gathered during this experiment—the developers’
  performances in source-code comprehension and bug identification and fixing.
  Another remarkable difference concerns the study of the affective reactions of
  participants while accomplishing these tasks.

  3     Controlled Experiment
  We followed the guidelines by Wohlin et al. [26] and Juristo and Moreno [12] to
  conduct our controlled experiment. The planning of this experiment follows the
  template suggested by Jedlitschka et al. [11].

  3.1     Goals
  We investigated the following main Research Question (RQ):
RQ1. Is there any difference when dealing with source code of native and cross-
     platform apps?
  We detailed RQ1 as follows:
        RQ1.1.    Is there any difference when dealing with native and cross-platform
        apps in   terms of source code comprehension?
        RQ1.2.    Is there any difference when dealing with native and cross-platform
        apps in   terms of bug identification?
        RQ1.3.    Is there any difference when dealing with native and cross-platform
        apps in   terms of bug fixing?
      We also studied the affective reactions of the participants when dealing with
  source-code comprehension and bug identification and fixing. To this end, we
  defined the following RQs:
4        Caulo, Maria et al.

      RQ2. Is there an effect on pleasure, arousal, dominance, and liking?
   A positive (or negative) effect of a technology with respect to these four
dimensions might imply that a developer is more (or less) effective when per-
forming the considered tasks. We deepened our investigation by focusing on
the difficulty the participants perceived when accomplishing experiment tasks.
Accordingly, we defined and studied also the following RQ:
      RQ3. Is there an effect on the difficulty the participants perceived when ac-
      complishing source code comprehension tasks and bug identification and fix-
      ing?

3.2     Experimental Units
Initially, 40 people accepted to take part in the experiment, but 39 actually par-
ticipated. The participants were students of the “Enterprise Mobile Applications
Development” course at the University of Salerno (Italy). This course focuses on
the study of Ionic-Cordova-Angular technologies. A few days before the exper-
iment, we asked the participants to fill in a pre-questionnaire. The goal was to
gather demographic information on the participants and their perspectives with
respect to points in favor and against cross-platform development.
    The average age of participants was 24. At the time of the experiment, par-
ticipants were 39 months (on average) experienced with programming and 10
months (on average) experienced with mobile programming, in particular. They
passed the programming exams with a rating of 27.4/30 on average. Most of
them attained the Mobile Development course focused on Android, in particular
35/39, and were rated 27/30, on average.
    The results of the pre-questionnaire also indicated that the participants were
in favor of cross-platform development for the following reasons: (i) simplicity of
development, (ii) complete abstraction of native programming languages, (iii)
“reusability” of the code, in the sense that they have to write it only once and
then they can deploy the app for the operative system(s) they need,(iv) develop-
ment speed, (v) wider number of reachable users, (vi) ease of maintenance,(vii)
big support in the development of graphical interfaces. The participants declared
the following points against the cross-platform development: (i) hard manage-
ment of native functionality (low control), (ii) big effort in finding a single best
possible solution valid for all operating systems, (iii) low performance (and lags),
(iv) wide occupation of resources, and (v) poor customization and no possible
use of specific features of a platform.

3.3     Experimental Material
As experimental objects, we used the source code of two versions of Movies-app:3
the original one—implemented by Ionic-Cordova-Angular technologies— and the
one migrated towards Android. Movies-app allows searching movies information,
3
    https://github.com/okode/movies-app
  Dealing with Comprehension and Bugs in Native and Cross-Platform Apps            5

then it is possible to filter the movies. Once selected a movie, it is possible to
have details on it and on its actors. We opted for this real-world app because (i)
it is not very complex (although not obvious), (ii) its problem domain can be
considered familiar with the participants, and (iii) it is small enough to allow a
good control over the participants that accomplished the tasks.
    We migrated the original version of Movies-app to Android using an approach
based on the most followed and well known one, named “chicken little” [2]:
 – Reverse Engineering, to analyze the project structure and identify the Ionic
   pages that could be grouped to implement a given functionality.
 – Migration planning, to define a migration order of the functionality and ser-
   vices. For each Ionic page, we performed three steps: (i) Pre-processing, (ii)
   GUI-Reengineering, and (iii) Single page and component code reengineering.
 – Data Reengineering, to store key/value pairs, files and SQLite data on the
   device file system.
 – Provider Reengineering, to map Ionic providers (i.e.,, services) into Java
   classes in Android.
 – Incremental integration and testing, to integrate each page with those of the
   same group. Starting from the Ionic app, test cases can be derived and used
   to exercise the developed apps and find eventual differences in the behavior.
    To gather affective reactions, we used the SAM [1] questionnaire, which con-
sists of a nine-point rating scale to evaluate pleasure, arousal, and dominance.
The pleasure scale ranges from “unhappiness/sadness” to “happiness/joyful-
ness”. The arousal scale ranges from “calm/bored” to “stimulated/excited”.
Finally, the dominance scale varies from “without control” to “with control”.
We also included the liking dimension. It consists of a nine-point rating scale:
from “dislike” to “like”. This further dimension is inspired by Koelstra et al. [16].
    We also asked participants to rate the level of difficulty (from one to five)
when comprehending source code and accomplishing a bug identification fixing
tasks. We used an approach similar to that by Scanniello et al. [20] in their
family of controlled experiments.

3.4   Tasks
We asked the participants to perform three tasks in the following order:
 1. Comprehension Task. We defined a comprehension questionnaire composed
    of six questions that admit open answers. We formulated these questions on
    the basis of the study by Sillito et.al. [23]. In particular, we picked the most
    asked by developers during change tasks and adapted them to our experi-
    mental material (i.e., the two apps). Sillito [23] organized such questions in
    four groups:
      – Finding Focus Point (FFP) aiming at finding points in the source code
         that were relevant to a given task.
      – Expanding Focus Point (EFP) aiming at expanding a given entity in
         the source code believed to be relevant often by exploring relationships
         among entities (e.g., classes and methods).
6           Caulo, Maria et al.

                                 Table 1. Comprehension questions.

ID                                             Questions                                              Category
    1 Where in the code is the text of the error message concerning the absence of popular movies?      FFP
    2 Where is there any code involved in the implementation of HTTP request to get the list of         FFP
      upcoming movies?
    3 Where is called the method that shows the description of a movie?                                 EFP
    4 What are the arguments to be given to the function that loads the detail of a movie?              EFP
    5 How does the list of movies resulting from the search by title look at runtime? (Indicate the     UAS
      code block responsible to display it)
    6 How can we know that the Persona data type (concerning the actor of a movie) has been created     QGS
      and initialized correctly in all its fields?




     – Understanding a Subgraph (UAS) aiming at building an understanding
        of concepts in the code that involved multiple relationships and entities.
     – Questions over a Group of Subgraphs (QGS) aiming at understanding
        the relationships between multiple subgraphs or understanding the in-
        teraction between a subgraph and the rest of the application.
   The questions of the comprehension questionnaire are shown in Table 1.
   This questionnaire was the same for both the groups of participants; those
   working with the Android version of the app and those working with the
   Ionic-Cordova-Angular version. We collected answers by a Google Form. For
   each question, we also asked the participants to provide the time when they
   start to work on a given question and the time when they believed to have
   correctly provided the answer. We did not force participants to provide an-
   swers to the questions.
2. Bug Identification. Similar to Scanniello et al. [21], we seeded (four) bugs in
   the source code of the two apps. We asked the participants to fix these bugs
   providing them with a bug report for each seeded one. The bug report was
   the same independently from the app version. The bug seeding was based on
   the mutation operators4 by Kim et al. [15]. We used the following operators:
     – Language Operator Replacement (LOR), that replaces a a language op-
        erator (e.g., <, >, <=, >=) with other legal alternatives.
     – Variable Replacement Operator (VRO), that replaces a variable name
        with other names of the same or compatible type(s).
     – Statements Swap Operator (SSO), that swaps contents of compatible
        blocks.
   A few details on the seeded bugs are shown in Table 2. An example of bug
   report for a seeded bug is shown in Table 3.
   We asked the participants to document where they believed each bug was
   in the source code. To this end, the participants had to write two lines of
   comment one before and another one after the statement/s containing the
   bug. The first line had to indicate the bug id as reported in the bug report.
   It is worth mentioning that we seeded the same bugs in both the versions
   of the apps. We show in Fig. 3.4 how the same bug (i.e., Search by Name
   of Authors not working) appears in the source code of the cross-platform
   version of the experimental object and its Android version.
4
     Mutation operators are predefined program modification rules [14].
  Dealing with Comprehension and Bugs in Native and Cross-Platform Apps                                                7

                                           Table 2. Seeded bugs

                                   Title in the Bug Reports           Mutation Operator Type
                         Search by Name of Authors not working                   LOR
                       Incorrect Value of the Duration of a movie                VRO
                                   No Actor’s Picture                            LOR
                    Second Star for movie Ratings wrongly Displayed               SSO


                         Table 3. A bug report used in the experiment

   Bug ID:        10348095
   Title:         “Search by Name of Authors not working”
                “The search field should return real-time clickable results, related to the name of the typed actor.
                For example, if I type the string ’Angelina’, the screen should display all the actresses
   Description: with that name in the results; if I complete with ’Jolie’, the screen should display
                a single result, which refers to the actress detail. Instead, when searching for an actor by
                name, the list of results is ALWAYS empty.”
   Submit date: 02/20/2019
   Author:        meryk90




 3. Bug Fixing. Participants had to fix the bugs they identified. We asked the
    participants to work with a bug at time. Bugs do not interfere one another.
 4. Post questionnaire. Participants had to fill in a post questionnaire. It includes
    a SAM questionnaire for each kind of task the participants accomplished
    (i.e., source-code comprehension, bug identification and bug fixing). We also
    asked the participants to assess the difficulty they perceived to execute these
    tasks.



3.5    Variables and Hypotheses

We considered one independent variable: Technology. It indicates the technology
used to implement the app. It is a categorical (or nominal) variable that can
assume the values of Android or Ionic.
    As far as source-code comprehension, we used the dependent variable Com-
prehension. It measures the correctness of understanding of a participant given
a version of Movies-app by analyzing the answers provided to the comprehen-
sion questionnaire. We used an approach based on that by Kamsties et al. [13]
that computes the number of correct responses to the questions of the com-
prehension questionnaire. We consider a response to a question to be correct if
the participant selected all the correct alternatives and no incorrect alternatives
were selected. The correct alternatives were defined before the experiment took
place. Comprehension assumes values between zero and six. A value close to six
means that a participant comprehended source code very well. A value close to
zero means that a participant obtained a low comprehension.
    A bug is successfully identified if the participant marks the source code where
the bug was seeded. We named the variable counting the bugs correctly identified
8       Caulo, Maria et al.




                                        (a)




                                        (b)

Fig. 1. Bug 10348095 in the 1(a) cross-platform and 1(b) Android versions of the app.



as Correctness of Bug Identification. This variable assumes values between zero
and four. The higher the value the better it is.
    As for bug fixing, we considered the variable: Correctness of Bug Fixing. It
counts the number of bugs the participants correctly fixed. A bug is correctly
fixed if the participant replaced the source code as it was before the application
of the mutation operator. In other words, the participants did the “undo” of
a given mutation operator we executed on the source code. In such a way we
assumed that there was only a way to fix each bug. Correctness of Bug Fixing
assumes values between zero and four. The higher the value the better it is.
   As for affective reactions, we considered four dependent variables (one for
each dimension of SAM plus the liking one) for each kind of task the participants
performed: comprehension, bug identification, and bug fixing. Therefore, the
dependent variables are: P LSK , ARSK , DOMK , and LIKK , where K indicates
the kind of task and assumes one of the following values: Comp (source code
comprehension), Ident (bug identification), and Fix (bug fixing). Each of the
twelve introduced dependent variables assumes values between zero and nine.
The best value is nine, while zero is the worst.
   We defined a dependent variable to measure the perceived difficulty for each
kind of task. As for the comprehension task, we defined Dif fComp . Similarly,
we defined Dif fIdent and Dif fF ix for the tasks bug identification and fixing,
respectively. All these three variables assume values between zero and five. The
higher the value, the better it is.
    To answer RQs, we tested the following parametrized null hypothesis.
  Dealing with Comprehension and Bugs in Native and Cross-Platform Apps            9

      H0X : There is no statistically significant difference between the participants
      who were administered with the cross-platform and the native versions of
      Movies-app with respect to X.

   X indicates one of the dependent variable described just before and then as-
sumes one of the following possible values: Comprehension (RQ1.1), Correctness
of Bug Identification (RQ1.2), Correctness of Bug Fixing (RQ1.3), P LSComp
(RQ2), ARSComp (RQ2), DOMComp (RQ2), LIKComp (RQ2), P LSIdent (RQ2),
ARSIdent (RQ2), DOMIdent (RQ2), LIKIdent (RQ2), P LSF ix (RQ2), ARSF ix
(RQ2), DOMF ix (RQ2), LIKF ix (RQ2), Dif fComp (RQ3), Dif fIdent (RQ3),
and Dif fF ix (RQ3).

3.6     Experiment Design
We used the one factor with two treatments design [26]. We randomly divided
the participants into two groups: Ionic and Android. The participants in the first
group were asked to accomplish the experiment tasks on the app implemented
by using the cross-platform technology, while those in the second group to on the
app implemented in the native technology. The participants in the Ionic group
were 20, while those in the Android one were 19.

3.7     Procedure
The experimental procedure included the following sequential steps.
1. We invited all the students of the Enterprise Mobile Applications Develop-
   ment course at the University of Salerno. They filled in a pre-questionnaire.
2. We randomly split the participants into: Ionic and Android.
3. The experiment session took place under controlled conditions in a labora-
   tory at the University of Salerno. The participants accomplished the tasks
   under the supervision of the authors to avoid any kind of interaction. All
   the used PCs had the same (Hardware/Software) configuration.
4. The participant performed the comprehension task by answering the ques-
   tions of the comprehension questionnaire.
5. We asked the participants to deal with each bug at time. The participants
   could pass to the next bug only when they either fixed the previous bug or
   were aware that they could not identify/fix it. Given a bug, participants first
   had to identify and mark the bug (as described before) and then they could
   pass to fix it.
6. Participants filled in the post-questionnaire by rating affective reactions and
   perceived difficulty.
7. Participants compressed and archived their version of the app with the
   source-code they modified. We then collected all those versions.
   Participants in the Android group could run the app either on the emulator of
Android Studio or on their own smartphone. Similarly, participants in the Ionic
group could run the app either on a web browser or on their own smartphone.
10         Caulo, Maria et al.

3.8      Analysis Procedure

To perform data analysis, we used the R environment5 for statistical computing
and we carried out the following steps:

 – We undertook the descriptive statistics of the dependent variables.
 – To test the hull hypotheses concerned to RQ1 we planned to use either
   an unpaired t-test or the Mann-Whitney U test [18]. Unlike the t-test, the
   Mann-Whitney U test does not require the assumption of normal distribu-
   tions. This is to say that if data are normally distributed we will apply the
   unpaired t-test, the Mann-Whitney U test otherwise. To study the normality
   of data, we use the Shapiro-Wilk W test [22]. In the case of a statistically
   significant effect of Technology, we plan to compute effect size to measure the
   magnitude of such a difference. If data are normally distributed we will opt
   for Cohen’s d, while Cliff’s δ otherwise. As for RQ2 and RQ3, we consider
   ordinal scales. Therefore, we could only apply a non-parametric statistical
   inferences. The Mann-Whitney U test [18] is the most appropriate.
 – To summarize and analyze raw data and to support their discussion, we ex-
   ploited different graphical representations: boxplots and clustered bar charts.

   To verify if an effect is statistically significant, we fixed α to 0.05. That is,
we admit 5% chance of a Type-I-error occurring [26]. If a p-value is less than
0.05, we deemed the effect is statistically significant.


4      Results and Discussion

In this subsection, we present and discuss the results according to our RQs.


4.1      RQ1: Native vs Cross-Platform Apps Concerning Source-Code
         Comprehension and Bug Identification and Fixing

In Table 4, we report the descriptive statistics for the dependent variables: Com-
prehension, Correctness of Bug Identification, and Correctness of Bug Fixing. In
this table, we also show the results of the statistical tests performed. To summa-
rize the distribution of these variables we used the boxplots shown in Fig. 4.1.
    As for Comprehension, descriptive statistics and boxplots do not show a
huge difference in the source-code comprehension the participants achieved in
the Ionic and Android groups. Descriptive statistics indicate that the partici-
pants in the Ionic group better answered the questions of the comprehension
questionnaire: the mean and median values are 0.625 and 0.667, respectively.
On the other hand, the mean and median values for the Android group are both
0.5. The results of the Shapiro-Wilk W test6 indicate that data are not nor-
mally distributed in the Ionic group (p-value = 0.007). For such a reason, we
5
     www.r-project.org
6
     A p-value less than alpha (i.e., 0.05) indicates that data are not normally distributed.
  Dealing with Comprehension and Bugs in Native and Cross-Platform Apps                                11




              (a)                                   (b)                               (c)
      Fig. 2. Boxplots for Comprehension, Correctness of Identification and Fixing.

Table 4. Descriptive Statistics for Comprehension, Correctness of Bug Identification
and Correctness of Bug Fixing dependent variables with respect to Technology.

                     Comprehension          Correctness of Bug Identification Correctness of Bug Fixing
Technology
             Mean Std. Dev. Median p-value Mean Std. Dev. Median   p-value   Mean Std. Dev. Median p-value
  Android     0.5     0.266    0.5          0.882   0.255   1                0.829   0.289    1
                                      0.109                         0.971                           0.935
   Ionic     0.625    0.152   0.667          0.9    0.189   1                0.850   0.235    1




performed the Mann-Whitney U test. As shown in Table 4, the p-value this test
returned is 0.109. That is, there is no statistically significant difference between
the comprehension that the participants in the two groups achieved.
     Concerning Correctness of Bug Identification, descriptive statistics (Table 4)
and boxplots (Figure 4.1) indicate that the participants in the groups (Android
and Ionic) achieved a high correctness in the identification of the bugs (e.g.,, the
mean value for Correctness of Bug Identification is 0.882 for Android and 0.9 for
Ionic) in both the versions of Movies-app. The data are also similarly distributed
and do not follow a normal distribution as the results of the Shapiro-Wilk W test
show (p-values are 1.294e-06 for Android and 1.422e-06 for Ionic). The results of
the Mann-Whitney U test do not (Table 4) indicate any statistically significant
difference between the data in the two groups since the p-value is 0.971.
     Finally, we observed a pattern similar to Correctness of Bug Identification
for Correctness of Bug Fixing. The participants in the groups achieved a high
correctness in the fixing of the bugs in both the versions of Movies-app. For
example, the mean value is 0.829 for Android, while 0.85 for Ionic. Data are
still not normally distributed. The Shapiro-Wilk W test returned the following
p-values for the groups Android and Ionic: 2.04e-05 and 2.656e-05. Therefore,
we applied the Mann-Whitney U test and we obtained 0.935 as the p-value.


4.2     RQ2: Native vs Cross-Platform Apps Concerning Pleasure,
        Arousal, Dominance, and Liking

As Sullivan and Artino [24] suggest, we used median values and frequencies as
descriptive statistics of the dependent variables P LSK , ARSK , DOMK , and
LIKK (where K is the kind of task). In Table 5, we report the median values
12          Caulo, Maria et al.

       Table 5. Median values for affective reactions and statistical test results.
Technology P LSComp ARSComp DOMComp LIKComp P LSIdent ARSIdent DOMIdent LIKIdent P LSF ix ARSF ix DOMF ix LIKF ix
 Android       7       7           8         7       8       7         8           8          8       7       8       8
  Ionic       7.5     6.5          9         8       8      7.5        9           8          8      7.5      8      8.5
 p-value     0.988   0.503       0.106     0.326   0.352   0.626     0.206       0.912      0.538   0.966   0.768   0.59




and the p-values of the statistical test performed, while we used the clustered
bar charts (Fig. 4.3) to show the frequencies.
    As for the Comprehension task, medians and clustered bar charts do not
show a wide difference between the dependent variables measured on the two
groups. However, medians of dependent variables for the Ionic group were al-
ways higher than the Android group ones (e.g., median of DOMComp is 8 for
Android and 9 for Ionic ), except for ARSComp (7 for Android and 6.5 for Ionic).
The Mann-Whitney U test returned p-values higher than 0.05 for all the depen-
dent variables, hence there is no statistically significant difference between the
affective reactions of both the groups of participants.
    Also for the Bug Identification task, median values and clustered bar charts
do not show a huge difference between the two groups. However, in this case,
medians of dependent variables of the Ionic group were always greater or equal
to the Android group ones (e.g., median of DOMComp is 8 for Android and 9
for Ionic). Also, the Mann-Whitney U test returned p-values higher than 0.05
for all the dependent variables signifying that there is no statistically significant
difference between the affective reactions of both the groups.
    The analysis of the Bug Fixing task follows the same pattern of the Bug
Identification task.

4.3        RQ3: Native vs Cross-Platform Concerning the Difficulty
In Table 6, we report the median values for Dif fComp , Dif fIdent , and Dif fF ix .
The p-values of the performed statistical tests are shown as well. The clustered
barcharts in Fig. 4.3 summarize the frequencies of the answers for task difficulty.
    Concerning all the three tasks, descriptive statistics do not show a difference
between the difficulty perceived by the participants of the two groups. Further-
more, medians are the same in the two groups for both Dif fComp and Dif fF ix
(4 and 5, respectively), while Dif fIdent has a slightly lower median for the Ionic
group (i.e., 4.5, while it is 5 for Android). The Mann-Whitney U test returned
p-values always higher than 0.05, hence there is no statistically significant dif-
ference between the difficulty perceived by the participants of both the groups.

             Table 6. Median values for difficulty with respect to Technology.

                                           Dif fComp       Dif fIdent        Dif fF ix
                             Technology
                                          Median p-value Median p-value Median p-value
                              Android        4              5                5
                                                   0.159           0.876                 0.789
                               Ionic         4             4.5               5
  Dealing with Comprehension and Bugs in Native and Cross-Platform Apps                 13




            (a)                            (b)                            (c)




            (d)                            (e)                            (f)



Fig. 3. Frequencies of the affective reactions for Android (Figg. 3(a), 3(b), and 3(c))
and Ionic (Figg. 3(d), 3(e), and 3(f)) groups.




                             (a)                         (b)



Fig. 4. Frequencies of the difficulty for Android (Fig. 4(a)) and Ionic (Fig. 4(b)) groups.


4.4   Implications and Future Extensions

We delineate main practical implications and future extension for our research.
14     Caulo, Maria et al.

 – Overall results suggest that the participants did not find any difference be-
   tween the two studied technologies with respect to source-code comprehen-
   sion and the correctness in identifying and fixing bugs. We also observed
   that also the affective reactions might not be affected by technology. In ad-
   dition, participants administered with the two treatments perceived difficulty
   in completing the tasks (i.e., source-code comprehension and bug identifica-
   tion and fixing) similarly. This outcome might be relevant to the practitioner.
   In particular, our study seems to support one of the main results from an
   industrial survey [8] that states that cross-platform development is valuable
   when an app has to be run in different hardware/software platforms.
 – Outcomes suggest future research on the design and the implementation of
   native and cross-platform apps. This point is of interest to the researcher.
 – The experiment object is of a specific kind of app, i.e., entertainment. The
   researcher and practitioner could be interested in studying whether our re-
   sults also hold for different kinds of app (e.g., games). Finally, it could be of
   interest for the researcher to study whether our outcomes scale to applica-
   tions more complex and larger.

5    Threats to Validity
We report threats to validity from the most to the least sensible. Since we were
more interested in studying cause-effect relationships, the most sensible kind of
threat is Internal Validity.
Internal Validity. A possible threat to Internal Validity is voluntary participa-
tion in the study (selection threat). However, we limited this threat by embedding
the experiment in a course at the University of Salerno and we did not consider
its outcome when grading the students. To deal with threat of diffusion or treat-
ments imitations, we monitored participants and asked back material to prevent
them from exchanging information. Another threat might be resentful demoral-
ization—participants assigned to a less desirable treatment might not perform
as good as they normally would.
Construct Validity. Each of the investigated constructs was quantified by
means of one assessment at the end of the task, which might affect the results
(i.e., mono-method bias threat). The participants were not informed about RQs.
However, they might guess them and change their behavior accordingly (i.e.,
threat of hypotheses guessing). To deal with this kind of threat (i.e., evaluation
apprehension threat), we did not evaluate the participants on the basis of their
performances. We also acknowledge the presence of a restricted generalizability
across constructs. That is, the technology can affect other relevant constructs
which we did not observe (cognitive load).
Conclusion Validity. To mitigate a threat of random heterogeneity of partici-
pants, our sample included students with a similar background. In particular, the
participants followed the same course at the same university, underwent similar
training, and had similar background, skills, and experience. Reliability of mea-
sures is another threat to conclusion validity. To deal with this kind of threat,
we used well known and widely used measures.
    Dealing with Comprehension and Bugs in Native and Cross-Platform Apps               15

External Validity. The participants in our study were graduate students. This
could pose some threats to the generalizability of the results to the popula-
tion of professional developers (threat of interaction of selection and treatment).
However, the use of students has the advantage that they have a homogeneous
background and are particularly suitable to obtain preliminary evidence [4].
Therefore, the use of students could be considered appropriate, as suggested in
the literature [4, 10]. In addition, the studied cross-platform technology is rela-
tively novel and then we can speculate that the participants are not so far from
many professional developers. The used experimental object might pose a threat
of interaction of setting and treatment.


6     Conclusion and Final Remarks

We presented the results of an experiment to investigate source-code comprehen-
sion of apps implemented either with native or cross-platform technologies. We
also investigated if these kinds of technology might play a role in the identifica-
tion and fixing of bugs in the source code. Furthermore, we studied the affective
reactions of participants and the difficulty they perceived when accomplishing
the tasks before mentioned. The data-analysis results suggested that there is
not a statistically significant difference in the comprehension and in the iden-
tification and fixing of bugs. This outcome holds also with respect to affective
reactions and the difficulty the participants perceived when accomplishing the
tasks.


References

 1. Bradley, M.M., Lang, P.J.: Measuring emotion: The self-assessment manikin and
    the semantic differential. Journal of Behavior Therapy and Experimental Psychi-
    atry 25(1), 49 – 59 (1994)
 2. Brodie, M.L., Stonebraker, M.: Legacy Information Systems Migration: Gateways,
    Interfaces, and the Incremental Approach. Morgan Kaufmann Publishers Inc., San
    Francisco, CA, USA (1995)
 3. Canfora, G., Di Penta, M.: New frontiers of reverse engineering. In: Proc. of Work-
    shop on the Future of Software Engineering. pp. 326–341. IEEE (2007)
 4. Carver, J., Jaccheri, L., Morasca, S., Shull, F.: Issues in using students in empirical
    studies in software engineering education. In: Proc. of Intl. Symposium on Software
    Metrics. pp. 239–251 (2003)
 5. Corral, L., Sillitti, A., Succi, G.: Mobile multiplatform development: An experiment
    for performance analysis. Procedia Computer Science 10, 736 – 743 (2012)
 6. Dalmasso, I., Datta, S.K., Bonnet, C., Nikaein, N.: Survey, comparison and eval-
    uation of cross platform mobile application development tools. In: Proc. of Intl.
    Wireless Communications and Mobile Computing Conference. pp. 323–328 (2013)
 7. El-Kassas, W.S., Abdullah, B.A., Yousef, A.H., Wahba, A.M.: Taxonomy of cross-
    platform mobile applications development approaches. Ain Shams Engineering
    Journal 8(2), 163–190 (2017)
                         16      Caulo, Maria et al.

                          8. Francese, R., Gravino, C., Risi, M., Scanniello, G., Tortora, G.: Mobile app de-
                             velopment and management: Results from a qualitative investigation. In: Proc. of
                             Intl. Conference on Mobile Software Engineering and Systems. pp. 133–143 (2017)
                          9. Heitkötter, H., Hanschke, S., Majchrzak, T.A.: Evaluating cross-platform develop-
                             ment approaches for mobile applications. In: Proc of. Web Information Systems
                             and Technologies. pp. 120–138. Springer Berlin Heidelberg (2013)
                         10. Höst, M., Regnell, B., Wohlin, C.: Using students as subjects—a comparative study
                             of students and professionals in lead-time impact assessment. Empirical Software
                             Engineering 5(3), 201–214 (2000)
                         11. Jedlitschka, A., Ciolkowski, M., Pfahl, D.: Reporting experiments in software en-
                             gineering. In: In Guide to Advanced Empirical Software Engineering. pp. 201–228.
                             Springer (2008)
                         12. Juristo, N., Moreno, A.: Basics of Software Engineering Experimentation. Kluwer
                             Academic Publishers (2001)
                         13. Kamsties, E., von Knethen, A., Reussner, R.: A controlled experiment to evaluate
                             how styles affect the understandability of requirements specifications. Information
                             & Software Technology 45(14), 955–965 (2003)
                         14. Kim, S., Clark, J., A. Mcdermid, J.: Class mutation: Mutation testing for object-
                             oriented programs (10 2000)
                         15. Kim, S., Clark, J.A., McDermid, J.A.: The rigorous generation of java mutation
                             operators using hazop technical report (1999)
                         16. Koelstra, S., Muhl, C., Soleymani, M., Lee, J., Yazdani, A., Ebrahimi, T., Pun, T.,
                             Nijholt, A., Patras, I.: Deap: A database for emotion analysis using physiological
                             signals. IEEE Transactions on Affective Computing 3(1), 18–31 (Jan 2012)
                         17. Malavolta, I., Ruberto, S., Soru, T., Terragni, V.: End users’ perception of hy-
                             brid mobile apps in the google play store. In: Proc. of Intl. Conference on Mobile
                             Services. pp. 25–32 (2015)
                         18. Mann, H.B., Whitney, D.R.: On a test of whether one of two random variables is
                             stochastically larger than the other. Ann. Math. Statist. 18(1), 50–60 (03 1947)
                         19. Que, P., Guo, X., Zhu, M.: A comprehensive comparison between hybrid and native
                             app paradigms. In: Proc. of Intl. Conference on Computational Intelligence and
                             Communication Networks. pp. 611–614 (Dec 2016)
                         20. Scanniello, G., Gravino, C., Risi, M., Tortora, G., Dodero, G.: Documenting design-
                             pattern instances: A family of experiments on source-code comprehensibility. ACM
                             Trans. Softw. Eng. Methodol. 24(3), 14:1–14:35 (2015)
                         21. Scanniello, G., Risi, M., Tramontana, P., Romano, S.: Fixing faults in C and java
                             source code: Abbreviated vs. full-word identifier names. ACM Trans. Softw. Eng.
                             Methodol. 26(2), 6:1–6:43 (2017)
                         22. Shapiro, S., Wilk, M.: An analysis of variance test for normality. Biometrika 52(3-
                             4), 591–611 (1965)
                         23. Sillito, J., Murphy, G.C., De Volder, K.: Asking and answering questions during a
                             programming change task. IEEE Trans. Softw. Eng. 34(4), 434–451 (2008)
                         24. Sullivan, G.M., Artino, A.R.: Analyzing and interpreting data from likert-type
                             scales. Journal of graduate medical education 5 4, 541–2 (2013)
                         25. Willocx, M., Vossaert, J., Naessens, V.: Comparing performance parameters of
                             mobile app development strategies. In: Proc. of Intl. Conference on Mobile Software
                             Engineering and Systems. pp. 38–47 (May 2016)
                         26. Wohlin, C., Runeson, P., Höst, M., Ohlsson, M., Regnell, B., Wesslén, A.: Experi-
                             mentation in Software Engineering. Springer (2012)




View publication stats
